# -*- coding: utf-8 -*-
"""Merging Datasets and Analyzing Models_Aggregated By Day.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jug9MQx0V5DTg-c6iZFkbRkko8IdK34N
"""

pip install pmdarima

# Import libraries
import numpy as np
import pandas as pd
import pandas_datareader as pdr
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import statsmodels.api as sm
import statsmodels.formula.api as smf
import seaborn as sns
import yfinance as yf
from statsmodels.tsa.api import VAR
from sklearn.preprocessing import StandardScaler
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.stattools import grangercausalitytests as gct
from statsmodels.stats.stattools import durbin_watson
from statsmodels.stats.outliers_influence import variance_inflation_factor
import os
from google.colab import drive

# Import pmdarima library
import pmdarima as pm
from pmdarima import utils
from pmdarima import arima
from pmdarima import model_selection
from statistics import mean
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_percentage_error

drive.mount('/content/drive/')
path = '/content/drive/MyDrive/blockchain'
os.chdir(path)

"""### 1. Our CSVs"""

blocks = pd.read_csv('./blocks.csv')
blocks

trans = pd.read_csv('./transcations.csv')
trans

df = pd.merge(blocks, trans, on=['block_number','date_time'], how='left')
df

df.dtypes

df.fillna(0, inplace=True)
df.sort_values(by='block_number', inplace=True)
df.drop(columns='block_number', inplace=True)
df['date_time'] = pd.to_datetime(blocks['date_time'].str[:16])
df.set_index('date_time', inplace=True)
by_day = df.resample('D').mean()
by_day.columns = ['avg_size','avg_gas_limit','avg_gas_used','avg_transaction_count', 'avg_transferred_eth']

# 1-period lag
by_day['avg_gas_used_lag1'] = by_day['avg_gas_used'].shift(1)
by_day['avg_transaction_count_lag1'] = by_day['avg_transaction_count'].shift(1)

# rolling average of the past 7 days
by_day['avg_gas_used_rolling6'] = by_day['avg_gas_used'].rolling(window=7).mean()
by_day['vg_transaction_count_rolling6'] = by_day['avg_transaction_count'].rolling(window=7).mean()

by_day.dropna(inplace=True)

by_day.reset_index(inplace=True)
by_day.rename(columns={'index': 'date_time'}, inplace=True)
by_day

#In order to predict tomorrow we need to lag it such that we can have tomorrow's gas used in today's row
by_day['avg_gas_used_tom'] = by_day['avg_gas_used'].shift(-1)
by_day.dropna(inplace=True)
by_day

#Interest Rates CSV
rates = pd.read_csv('./long_term_rates_2000_2023.csv')
rates.drop('Extrapolation Factor', axis=1, inplace=True)

rates['Date'] = pd.to_datetime(rates['Date'])
rates.rename(columns={'LT COMPOSITE (>10 Yrs)': 'Interest_rate'}, inplace=True)
rates

df = pd.merge(by_day, rates, how='right', left_on='date_time', right_on='Date')
#df.set_index('date_time', inplace=True)
df.sort_values(by='Date', inplace=True)
df.dropna(inplace=True)
df.drop('Date', axis=1, inplace=True)

df



"""### 2. Plot data"""

# download cryptocurrency data
btc = yf.download('BTC-USD', start='2021-08-06', end='2023-12-31')
eth = yf.download('ETH-USD', start='2021-08-06', end='2023-12-29')

btc.reset_index(inplace=True)
btc.head()

eth.reset_index(inplace=True)
eth.head()

plt.figure(figsize=(10, 6))
plt.plot(df['date_time'], df['avg_gas_used'], label='Avg daily Unit of Gas Used')  # Assuming date_time is the index
plt.title('Avg Daily Unit of Gas Used')
plt.xlabel('Date')
plt.ylabel('Unit of Gas')
plt.legend()
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(10, 6))
plt.plot(eth['Date'], eth['Adj Close'], label='ETH-USD Price')  # Assuming the date is the index for eth
plt.title('ETH-USD Price')
plt.xlabel('Date')
plt.ylabel('Price')
plt.legend()
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(10, 6))
plt.plot(eth['Date'], eth['Volume'], label='ETH-USD Volume')  # Assuming the date is the index for eth
plt.title('ETH-USD Volume')
plt.xlabel('Date')
plt.ylabel('Volume')
plt.legend()
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(10, 6))
plt.plot(btc['Date'], btc['Adj Close'], label='BTC-USD Price')  # Assuming the date is the index for btc
plt.title('BTC-USD Price')
plt.xlabel('Date')
plt.ylabel('Price')
plt.legend()
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(10, 6))
plt.plot(btc['Date'], btc['Volume'], label='BTC-USD Volume')  # Assuming the date is the index for btc
plt.title('BTC-USD Volume')
plt.xlabel('Date')
plt.ylabel('Volume')
plt.legend()
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(10, 6))
plt.plot(df['date_time'], df['avg_transaction_count'], label='Avg Transaction Count')  # Assuming date_time is the index
plt.title('Avg Transaction Count')
plt.xlabel('Date')
plt.ylabel('Count')
plt.legend()
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(10, 6))
plt.plot(df['date_time'], df['avg_size'], label='Avg Block Size per Day')  # Assuming date_time is the index
plt.title('Avg Block Size')
plt.xlabel('Date')
plt.ylabel('Size')
plt.legend()
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(10, 6))
plt.plot(df['date_time'], df['Interest_rate'], label='10-year real IR')  # Assuming date_time is the index
plt.title('10-year real IR')
plt.xlabel('Date')
plt.ylabel('Interest Rate')
plt.legend()
plt.xticks(rotation=45)
plt.show()



"""### 3. Merge All CSVs"""

#Merge df & Eth Together
eth_renamed = eth.rename(columns={col: col + '_eth' if col != 'Date' else col for col in eth.columns})

df_plus_eth = pd.merge(df, eth_renamed, how='left', left_on='date_time', right_on='Date')

df_plus_eth

#Merge df_plus_eth & Bit Together
btc_renamed = btc.rename(columns={col: col + '_btc' if col != 'Date' else col for col in btc.columns})

df_all = pd.merge(df_plus_eth, btc_renamed, how='left', left_on='date_time', right_on='Date')

df_all

"""### 4. Heatmap"""

# plot the correlation coefficient
correlation_matrix = df_all.corr()

# Create a heatmap using seaborn
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")

# Set the title
plt.title('Correlation Heatmap')

# Show the plot
plt.show()

df_all.columns

df_all.set_index('date_time', inplace=True)

df_all = df_all.drop(['Date_x', 'Date_y'], axis=1)

"""#### Linear Regression with Backtest"""



#CORRECTED LINEAR REGRESSION W/ PRINT MIDWAY
import pandas as pd
import numpy as np
import statsmodels.api as sm

window_size = 30

actuals = []
predictions = []

# Loop through the DataFrame
for t in range(window_size, len(df_all)):
    # Define the training window
    train = df_all.iloc[t - window_size:t]

    # The independent variables (features) should exclude the target variable 'avg_gas_used'
    # This assumes all other columns are features. If not, select the appropriate columns
    X_train = train.drop(columns=['avg_gas_used_tom'])
    y_train = train['avg_gas_used_tom']

    # Add a constant to the model (the intercept term)
    X_train = sm.add_constant(X_train)

    # Fit the model
    model = sm.OLS(y_train, X_train).fit()

    # Prepare X_test for the next day, ensuring it is 2D and has the same structure as X_train
    test_features = df_all.iloc[t]  # Get the features for day t
    test_features = test_features.drop(labels=['avg_gas_used_tom'])  # Drop the target variable
    X_test = pd.DataFrame([test_features])  # Make it 2D
    X_test = sm.add_constant(X_test)  # Add the constant

    print(X_train)

#CORRECTED LINEAR REGRESSION
import pandas as pd
import numpy as np
import statsmodels.api as sm

window_size = 30

actuals = []
predictions = []

# Loop through the DataFrame
for t in range(window_size, len(df_all)):
    # Define the training window
    train = df_all.iloc[t - window_size:t-1] # t-1

    # The independent variables (features) should exclude the target variable 'avg_gas_used'
    # This assumes all other columns are features. If not, select the appropriate columns
    X_train = train.drop(columns=['avg_gas_used_tom'])
    y_train = train['avg_gas_used_tom']

    # Add a constant to the model (the intercept term)
    X_train = sm.add_constant(X_train)

    # Fit the model
    model = sm.OLS(y_train, X_train).fit()

    # Prepare X_test for the next day, ensuring it is 2D and has the same structure as X_train
    test_features = df_all.iloc[t]  # Get the features for day t
    test_features = test_features.drop(labels=['avg_gas_used_tom'])  # Drop the target variable
    X_test = pd.DataFrame([test_features])  # Make it 2D
    X_test = sm.add_constant(X_test, has_constant='add')  # Add the constant

    #print(X_test)

    # Predict the next day's avg_gas_used
    pred = model.predict(X_test)[0]
    # Store the actual and predicted values
    actual = df_all['avg_gas_used_tom'].iloc[t]  # Actual value for day t (actually need to use t+1)
    actuals.append(actual)
    predictions.append(pred)

# Now, actuals and predictions hold the actual and predicted values, respectively
# You can calculate performance metrics like MSE and R-squared here

# Import mean_squared_error and r2_score if not already imported
from sklearn.metrics import mean_squared_error, r2_score

actuals = actuals[:-1]
predictions = predictions[:-1]

mse = mean_squared_error(actuals, predictions) # double check Y_hat_t+1 and Y_t+1 are on the same row (row t), and drop nulls
r2 = r2_score(actuals, predictions)

print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')

pd.DataFrame(predictions).shift()

# X_t, Y_t+1
# t: X_t -> Y_hat_t+1 vs Y_t+1, on row t because you led the target

"""#### XGBRegressor"""

df_all.dtypes

#With Correct Back Test
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Adjusting window size to 7 days as requested
window_size = 7

actuals = []
predictions = []

# Loop through the DataFrame
for t in range(window_size, len(df_all-1)):
    # Define the training window
    train = df_all.iloc[t - window_size:t-1]

    # The independent variables (features) should exclude the target variable 'avg_gas_used_tom'
    X_train = train.drop(columns=['avg_gas_used_tom'])
    y_train = train['avg_gas_used_tom']

    # Initialize and fit the XGBoost model
    model = XGBRegressor()
    model.fit(X_train, y_train)

    # Prepare X_test for the next day, ensuring it has the same structure as X_train
    test_features = df_all.iloc[t]  # Get the features for day t
    X_test = test_features.drop(labels=['avg_gas_used_tom']).to_frame().T  # Drop target and convert to DataFrame

    # Predict the next day's avg_gas_used
    pred = model.predict(X_test)[0]

    # Store the actual and predicted values
    actual = df_all['avg_gas_used_tom'].iloc[t]
    actuals.append(actual)
    predictions.append(pred)

actuals = actuals[:-1]
predictions = predictions[:-1]

# Calculate performance metrics like MSE and R-squared
mse = mean_squared_error(actuals, predictions)
r2 = r2_score(actuals, predictions)

print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')

"""#### Random Forest Regression"""

#### With backtest

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Adjusting window size to 7 days as requested
window_size = 7

actuals = []
predictions = []

# Loop through the DataFrame
for t in range(window_size, len(df_all)-1):
    # Define the training window
    train = df_all.iloc[t - window_size:t]

    # The independent variables (features) should exclude the target variable 'avg_gas_used_tom'
    X_train = train.drop(columns=['avg_gas_used_tom'])
    y_train = train['avg_gas_used_tom']

    # Initialize and fit the Random Forest model
    model = RandomForestRegressor()
    model.fit(X_train, y_train)

    # Prepare X_test for the next day, ensuring it has the same structure as X_train
    test_features = df_all.iloc[t]  # Get the features for day t
    X_test = test_features.drop(labels=['avg_gas_used_tom']).to_frame().T  # Drop target and convert to DataFrame

    # Predict the next day's avg_gas_used
    pred = model.predict(X_test)[0]

    # Store the actual and predicted values
    actual = df_all['avg_gas_used_tom'].iloc[t]
    actuals.append(actual)
    predictions.append(pred)

actuals = actuals[:-1]
predictions = predictions[:-1]

# Calculate performance metrics like MSE and R-squared
mse = mean_squared_error(actuals, predictions)
r2 = r2_score(actuals, predictions)

print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')

"""#### K-Nearest Neighbors"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Adjusting window size to 7 days as requested
window_size = 7

actuals = []
predictions = []

# Loop through the DataFrame
for t in range(window_size, len(df_all)-1):
    # Define the training window
    train = df_all.iloc[t - window_size:t]

    # The independent variables (features) should exclude the target variable 'avg_gas_used'
    X_train = train.drop(columns=['avg_gas_used_tom'])
    y_train = train['avg_gas_used_tom']

    # Initialize and fit the K-Nearest Neighbors model
    model = KNeighborsRegressor(n_neighbors=5)  # n_neighbors is a parameter you may want to tune
    model.fit(X_train, y_train)

    # Prepare X_test for the next day, ensuring it has the same structure as X_train
    test_features = df_all.iloc[t]  # Get the features for day t
    X_test = test_features.drop(labels=['avg_gas_used_tom']).to_frame().T  # Drop target and convert to DataFrame

    # Predict the next day's avg_gas_used
    pred = model.predict(X_test)[0]

    # Store the actual and predicted values
    actual = df_all['avg_gas_used_tom'].iloc[t]  # Actual value for day t+1
    actuals.append(actual)
    predictions.append(pred)

# Calculate performance metrics like MSE and R-squared
mse = mean_squared_error(actuals, predictions)
r2 = r2_score(actuals, predictions)

print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')



"""#### Ridge Model"""

from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, r2_score

# Adjusting window size to 7 days as requested
window_size = 7

actuals = []
predictions = []

# Loop through the DataFrame
for t in range(window_size, len(df_all)-1):
    # Define the training window
    train = df_all.iloc[t - window_size:t]

    # The independent variables (features) should exclude the target variable 'avg_gas_used'
    X_train = train.drop(columns=['avg_gas_used'])
    y_train = train['avg_gas_used']

    # Initialize and fit the Ridge regression model
    model = Ridge(alpha=1.0)  # Alpha is a regularization parameter that you may tune
    model.fit(X_train, y_train)

    # Prepare X_test for the next day, ensuring it has the same structure as X_train
    test_features = df_all.iloc[t]  # Get the features for day t
    X_test = test_features.drop(labels=['avg_gas_used']).to_frame().T  # Drop target and convert to DataFrame

    # Predict the next day's avg_gas_used
    pred = model.predict(X_test)[0]

    # Store the actual and predicted values
    actual = df_all['avg_gas_used'].iloc[t+1]  # Actual value for day t+1
    actuals.append(actual)
    predictions.append(pred)

# Calculate performance metrics like MSE and R-squared
mse = mean_squared_error(actuals, predictions)
r2 = r2_score(actuals, predictions)

print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')

"""#### Arima Model"""

from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_squared_error

# Adjusting window size to 7 days as requested
window_size = 7

actuals = []
predictions = []

# Loop through the DataFrame
for t in range(window_size, len(df_all)-1):
    # Define the training window
    train = df_all.iloc[t - window_size:t]

    # The target variable 'avg_gas_used'
    y_train = train['avg_gas_used']

    # Initialize and fit the ARIMA model
    # Assuming an ARIMA(1,1,1) model; you might need to optimize the order (p,d,q) based on your data
    model = ARIMA(y_train, order=(1,1,1))
    model_fit = model.fit()

    # Predict the next day's avg_gas_used
    pred = model_fit.forecast()[0]

    # Store the actual and predicted values
    actual = df_all['avg_gas_used'].iloc[t+1]  # Actual value for day t+1
    actuals.append(actual)
    predictions.append(pred)

# Calculate performance metrics like MSE
mse = mean_squared_error(actuals, predictions)

print(f'Mean Squared Error: {mse}')

"""#### Transformers"""

from transformers import pipeline

classifier = pipeline(task = "time-series-foundation-models/Lag-Llama")

classifier

from transformers import pipeline

model_name = "time-series-foundation-models/Lag-Llama"

classifier = pipeline(task = "predict tomorrow", model = model_name)

classifier

from transformers import AutoformerConfig, AutoformerModel

# Initializing a default Autoformer configuration
configuration = AutoformerConfig()

# Randomly initializing a model (with random weights) from the configuration
model = AutoformerModel(configuration)

# Accessing the model configuration
configuration = model.config

from huggingface_hub import hf_hub_download
import torch
from transformers import AutoformerModel

file = hf_hub_download(
    repo_id="hf-internal-testing/tourism-monthly-batch", filename="train-batch.pt", repo_type="dataset"
)
batch = torch.load(file)

model = AutoformerModel.from_pretrained("huggingface/autoformer-tourism-monthly")

# during training, one provides both past and future values
# as well as possible additional features
outputs = model(
    past_values=batch["past_values"],
    past_time_features=batch["past_time_features"],
    past_observed_mask=batch["past_observed_mask"],
    static_categorical_features=batch["static_categorical_features"],
    future_values=batch["future_values"],
    future_time_features=batch["future_time_features"],
)

last_hidden_state = outputs.last_hidden_state



"""#### Disregard below it is incorrect

### 5. OLS Regression Analysis with Backtest
"""

import pandas as pd
import numpy as np
import statsmodels.formula.api as smf
from sklearn.metrics import mean_squared_error, r2_score

# Define the formula
formula = 'avg_gas_used ~ avg_size + avg_gas_limit + avg_transaction_count + avg_transferred_eth + avg_gas_used_lag1 + avg_transaction_count_lag1 + avg_gas_used_rolling6 + vg_transaction_count_rolling6 + Interest_rate + Close_eth + Volume_eth + Close_btc + Volume_btc'

# Initialize lists for actual values and predictions
actuals = []
predictions = []

# Define the window size for the rolling prediction
window_size = 30

# Rolling prediction
for end in range(window_size, len(df_all)):
    # Define the training data up to the current point
    train_data = df_all.iloc[:end]
    # The current point to predict
    test_data = df_all.iloc[end:end+1]

    if test_data.isnull().values.any():
        # Skip this iteration if there are NaN values in the test_data
        continue

    # Fit the model on the training data
    model_eth = smf.ols(formula=formula, data=train_data.dropna()).fit()

    # Predict the current day
    pred = model_eth.predict(test_data)

    # Check if pred is NaN
    if not np.isnan(pred.values[0]):
        predictions.append(pred.values[0])
        actuals.append(test_data['avg_gas_used'].values[0])

# Ensure actuals and predictions lists are not empty before calculating metrics
if actuals and predictions:
    mse = mean_squared_error(actuals, predictions)
    r2 = r2_score(actuals, predictions)

    print(f'Mean Squared Error: {mse}')
    print(f'R-squared: {r2}')
else:
    print("No valid predictions were made.")

"""Notes:
* The R-squared value is very high at 0.989, indicating that the model explains 98.9% of the variability in the dependent variable. The Adjusted R-squared is 0.989 as well, which is very close to the R-squared value, suggesting that the number of predictors is appropriate for the number of observations.

* The F-statistic is 3625, which is very large, and the Prob (F-statistic) is 0.00, indicating that the model is statistically significant at a general level.

* The condition number is very high (9.08e+13), indicating potential multicollinearity issues.
"""

# Does Not include backtesting!!
#Drop avg_size, avg_gas_limit, avg_transaction_count_lag1, Interest_rate, Volume_eth, Volume_btc
# ReFit the model
formula = 'avg_gas_used ~ avg_transaction_count + avg_transferred_eth + avg_gas_used_lag1 + avg_gas_used_rolling6 + vg_transaction_count_rolling6 + Close_eth + Close_btc '

# Running the OLS regression on df_all after dropping missing values
model_eth = smf.ols(formula=formula, data=df_all.dropna())
result_eth = model_eth.fit()

# Display the regression summary
result_eth.summary()

#THIS DOES NOT INCLUDE BACKTESTING!!
import statsmodels.formula.api as smf

formula = 'avg_gas_used ~ avg_size + avg_gas_limit + avg_transaction_count + avg_transferred_eth + avg_gas_used_lag1 + avg_transaction_count_lag1 + avg_gas_used_rolling6 + vg_transaction_count_rolling6 + Interest_rate + Close_eth + Volume_eth + Close_btc + Volume_btc'

# Running the OLS regression on df_all after dropping missing values
model_eth = smf.ols(formula=formula, data=df_all.dropna())
result_eth = model_eth.fit()

# Display the regression summary
result_eth.summary()

# Check multicollinearity
X = result_eth.model.exog
vif = [variance_inflation_factor(X,i) for i in range(1,X.shape[1])]
vif

"""We see that a couple have very high multi-colinearity so we would probably only move forward wth those with under 10 for their VIF scores"""

actuals = []
predictions = []

# Window size for training
window_size = 30  # 30 days

# Iterate over the DataFrame, training and predicting one step ahead each time
for end in range(window_size, len(df_all)):
    # Define the training window up to the current time step, excluding the current day
    train_data = df_all.iloc[end-window_size:end]
    # Define the test point (the current day)
    test_data = df_all.iloc[[end]]

    # Separate features and target variable
    X_train = train_data.drop('avg_gas_used', axis=1)
    y_train = train_data['avg_gas_used']
    X_test = test_data.drop('avg_gas_used', axis=1)
    y_test = test_data['avg_gas_used']

    # Initialize and train the XGBoost regressor
    xgb_model = xgb.XGBRegressor(objective='reg:squarederror')
    xgb_model.fit(X_train, y_train)

    # Predict the current day's avg_gas_used
    y_pred = xgb_model.predict(X_test)

    # Append the actual and predicted values to their respective lists
    actuals.append(y_test.values[0])
    predictions.append(y_pred[0])

# Calculate performance metrics such as Mean Squared Error and R-squared
mse = mean_squared_error(actuals, predictions)
r2 = r2_score(actuals, predictions)

print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')

actuals_array = np.array(actuals)
predictions_array = np.array(predictions)

# Plot the actual vs predicted values
plt.figure(figsize=(14, 7))
plt.plot(actuals_array, label='Actual avg_gas_used', color='blue', marker='o')
plt.plot(predictions_array, label='Predicted avg_gas_used', color='red', linestyle='--', marker='x')
plt.title('Actual vs Predicted avg_gas_used')
plt.xlabel('Time Steps')
plt.ylabel('avg_gas_used')
plt.legend()
plt.show()

"""* It seems like the r-squared is way too high so there is probably an issue with the code or some look-ahead bias?"""

#I think that the below code has look-ahead bias and it was what we presented in class

import xgboost as xgb

data = df_all

training_part = data.iloc[:int(len(data)*0.8),:]
testing_part = data.iloc[int(len(data)*0.8):,:]

window_size = 30  # 30 days
label = []
pred = []

for i in range(window_size, len(training_part)):
    train_data = training_part.iloc[i-window_size:i]
    val_data = training_part.iloc[i:i+1]

    X_train = train_data.drop('avg_gas_used', axis=1)
    y_train = train_data['avg_gas_used']
    X_val = val_data.drop('avg_gas_used', axis=1)
    y_val = val_data['avg_gas_used']

    xgb_model = xgb.XGBRegressor()
    xgb_model.fit(X_train, y_train)
    y_pred = xgb_model.predict(X_val)

    label.append((y_val.index[0],y_val.values))
    pred.append(y_pred)

date_time = []
values = []
for record in label:
  date_time.append(record[0])
  values.append(record[1])

y_val_label = pd.Series(values, index=date_time)
y_val_pred = pd.Series(pred, index=date_time)

plt.plot(y_val_label.index, y_val_label.values, label='Label')
plt.plot(y_val_pred.index, y_val_pred.values, label='Prediction')

plt.xlabel('Date')
plt.xticks(rotation=45)
plt.ylabel('Avg Gas Used')
plt.title('Label vs Prediction')

plt.legend()

plt.show()

X_test = testing_part.drop('avg_gas_used', axis=1)
y_test = testing_part['avg_gas_used']
y_pred = xgb_model.predict(X_test)
y_pred = pd.Series(y_pred, index=y_test.index)

plt.plot(y_test.index, y_test.values, label='Label')
plt.plot(y_pred.index, y_pred.values, label='Prediction')

plt.xlabel('Date')
plt.xticks(rotation=45)
plt.ylabel('Avg Gas Used')
plt.title('Label vs Prediction')

plt.legend()

plt.show()

from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
mse = mean_squared_error(y_test, y_pred)
print(f'MSE: {mse}')
r2 = r2_score(y_test, y_pred)
print(f"R-squared: {r2}")
# Not sure we r squared is negative

"""#### Random Forest"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Assuming df_all is your DataFrame and it's sorted chronologically
# Also assuming that 'avg_gas_used' is the column you're predicting

# Initialize lists for actual values and predictions
actuals = []
predictions = []

# Window size for training
window_size = 30  # 30 days

# Iterate over the DataFrame, training and predicting one step ahead each time
for end in range(window_size, len(df_all)):
    # Define the training window up to the current time step, excluding the current day
    train_data = df_all.iloc[end-window_size:end]
    # Define the test point (the current day)
    test_data = df_all.iloc[[end]]

    # Separate features and target variable
    X_train = train_data.drop('avg_gas_used', axis=1)
    y_train = train_data['avg_gas_used']
    X_test = test_data.drop('avg_gas_used', axis=1)
    y_test = test_data['avg_gas_used']

    # Initialize and train the Random Forest regressor
    rf_model = RandomForestRegressor()
    rf_model.fit(X_train, y_train)

    # Predict the current day's avg_gas_used
    y_pred = rf_model.predict(X_test)

    # Append the actual and predicted values to their respective lists
    actuals.append(y_test.values[0])
    predictions.append(y_pred[0])

# Calculate performance metrics
mse = mean_squared_error(actuals, predictions)
r2 = r2_score(actuals, predictions)

print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')









# Tried this but not really sure what it does

"""### 4. Simple ARIMA model (Only use gas_used)"""

# Decompose data (trend * seasonality * randomness)
utils.decomposed_plot(arima.decompose(df_all['avg_gas_used'],'additive',m=12),
                      figure_kwargs = {'figsize': (12, 12)} )

plt.rcParams['figure.figsize'] = [12, 3]

# Plot Autocorrelation and Partial Autocorrelation
# Since the data is non-stationary, the graphs are so weired.
utils.plot_acf(df_all['avg_gas_used'], alpha=.05)
utils.plot_pacf(df_all['avg_gas_used'], alpha=.05)





















####DON'T LOOK BELOW THIS IT IS ALL WRONG!

# To chage the data from non-stationary to stationary, let's try to take the difference.
# Check how many times I should take the difference (normal diff and seasonal diff)
print('d =', arima.ndiffs(merge['AVG_Unit_of_Gas']))
print('D =', arima.nsdiffs(merge['AVG_Unit_of_Gas'],m=7))

# Take only one normal diff
avg_gas_used_d1 = merge['AVG_Unit_of_Gas'].diff(1).dropna()

plt.rcParams['figure.figsize'] = [8, 3]

# Let's re-plot Autocorrelation and Partial Autocorrelation
# now graphs make sense some what
utils.plot_acf(avg_gas_used_d1, alpha=.05)
utils.plot_pacf(avg_gas_used_d1 , alpha=.05)

# Split data
# The total number = 938, train data = 0.8*938≒785
train, test = model_selection.train_test_split(merge['AVG_Unit_of_Gas'], train_size=785)

# Let's create arema model automatically
arima_model = pm.auto_arima(train,
                            seasonal=True,
                            m=7,
                            d=1,
                            trace=True,
                            n_jobs=-1,
                            maxiter=10)

train_pred = arima_model.predict_in_sample()

test_pred, test_pred_ci = arima_model.predict(
    n_periods=test.shape[0],
    return_conf_int=True
)
print('MSE:')
print(mean_squared_error(test, test_pred))
print('RMSE:')
print(np.sqrt(mean_squared_error(test, test_pred)))
print('MAE:')
print(mean_absolute_error(test, test_pred))
print('MAPE:')
print(mean_absolute_percentage_error(test, test_pred))

plt.rcParams['figure.figsize'] = [8, 4]

preds, conf_int95 = arima_model.predict(n_periods=test.shape[0], return_conf_int=True,
                                       alpha=0.05)


# Check the difference between predicted data and actual
x_axis = np.arange(preds.shape[0])
plt.plot(x_axis,test,label="actual",color='black')
plt.plot(x_axis,preds,label="predicted",color='red')
plt.fill_between(x_axis[-preds.shape[0]:],
                 conf_int95[:, 0], conf_int95[:, 1],
                 alpha=0.05, color='b')
plt.legend()
plt.show()

x_axis = np.arange(train.shape[0] + preds.shape[0])
plt.plot(x_axis[:train.shape[0]],train,color='b',label="actual")
plt.plot(x_axis[train.shape[0]:],test,color='b')
plt.plot(x_axis[train.shape[0]:],preds,color='red',label="predicted")
plt.fill_between(x_axis[-preds.shape[0]:],
                 conf_int95[:, 0], conf_int95[:, 1],
                 alpha=0.05,color='red')
plt.legend()
plt.show()

"""### 5. Vector AutoRegressive model (use multiple AutoRegressive of each feature)"""

plt.rcParams['figure.figsize'] = [6, 4]
merge['AVG_Unit_of_Gas'].diff().dropna().plot()
plt.title('AVG_Unit_of_Gas_diff')

merge['ETH_USD_price'].diff().dropna().plot()
plt.title('ETH_USD_price')

merge['Number_of_blocks'].diff().dropna().plot(color = 'red')
plt.title('Number_of_blocks')

merge['Distinct_miner'].diff().dropna().plot(color = 'orange')
plt.title('Distinct_miner')

merge['AVG_block_size'].diff().dropna().plot(color = 'purple')
plt.title('AVG_block_size')

merge['ten_year_real_ir'].diff().dropna().plot(color = 'black')
plt.title('black')

merge['one_year_real_ir'].diff().dropna().plot(color = 'green')
plt.title('green')

# drop interest rate because there are too many 0 data
new_merge = merge.drop(columns=['ETH_USD_volume','BTC_USD_price','BTC_USD_volume','Avg_Transaction_Count','ten_year_real_ir','one_year_real_ir',],axis=1)
new_merge

var_model = VAR(new_merge.diff().dropna())

lag_order = var_model.select_order(maxlags=50)
print(lag_order.summary())

# Fit the model based on aic
optimal_lag = lag_order.selected_orders['aic']
var_model_fitted = var_model.fit(optimal_lag)

# Did durbin-watson test
# If each value is between 1.6 and 2.4, this regression is valid
out = durbin_watson(var_model_fitted.resid)

for col, val in zip(new_merge.diff().columns, out):
    print(col, ':', round(val, 2))

forecast_input = new_merge.diff().values[-optimal_lag:]

forecast_span = 20
fc = var_model_fitted.forecast(y=forecast_input, steps=forecast_span)
new_merge_forecast = pd.DataFrame(fc, index=new_merge.index[-forecast_span:], columns=new_merge.columns + '_1d')

# To transform the first-order difference model to the original scale
def invert_transformation(df_train, df_forecast, second_diff=False):
    """Revert back the differencing to get the forecast to original scale."""
    df_fc = df_forecast.copy()
    columns = df_train.columns
    for col in columns:
        # Roll back 2nd Diff
        if second_diff:
            df_fc[str(col)+'_1d'] = (df_train[col].iloc[-1]-df_train[col].iloc[-2]) + df_fc[str(col)+'_2d'].cumsum()
        # Roll back 1st Diff
        df_fc[str(col)+'_forecast'] = df_train[col].iloc[-1] + df_fc[str(col)+'_1d'].cumsum()
    return df_fc

df_results = invert_transformation(new_merge, new_merge_forecast, second_diff=False)

fig, axes = plt.subplots(nrows=2, ncols=2, dpi=100, figsize=(10,10))
for i, (col,ax) in enumerate(zip(new_merge.columns, axes.flatten())):
    #df_results[col+'_forecast'].plot(legend=True, ax=ax).autoscale(axis='x',tight=True)
    df_results[col+'_forecast'].plot(legend=True, ax=ax,grid=True)
    new_merge[col][-forecast_span:].plot(legend=True, ax=ax,grid=True);
    ax.set_title(col + ": Forecast vs Actuals")
    ax.xaxis.set_ticks_position('none')
    ax.yaxis.set_ticks_position('none')
    ax.spines["top"].set_alpha(0)
    ax.tick_params(labelsize=6)

plt.tight_layout()

from statsmodels.tsa.stattools import acf
def forecast_accuracy(forecast, actual):
    mape = np.mean(np.abs(forecast - actual)/np.abs(actual))  # MAPE
    mae = np.mean(np.abs(forecast - actual))    # MAE
    mpe = np.mean((forecast - actual)/actual)   # MPE
    mse = np.mean((forecast - actual)**2)      # MSE
    rmse = np.mean((forecast - actual)**2)**.5  # RMSE
    return({'mape':mape,'mae': mae,'mpe': mpe,'mse':mse ,'rmse':rmse})

print('Forecast Accuracy of: AVG_Unit_of_Gas')
accuracy_prod = forecast_accuracy(df_results['AVG_Unit_of_Gas_forecast'].values, new_merge.loc['2024-02-10':'2024-02-29','AVG_Unit_of_Gas'])
for k, v in accuracy_prod.items():
    print(k, ': ', round(v,4))

